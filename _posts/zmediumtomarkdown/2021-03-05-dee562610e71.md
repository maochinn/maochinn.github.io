---
title: "【筆記】Camera"
author: "帽捲"
date: 2021-03-05T10:32:33.589+0000
last_modified_at: 2021-03-16T05:21:41.206+0000
categories: ["Maochinn"]
tags: ["computer-graphics","pytorch3d","blender","opengl"]
description: "這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。"
image:
  path: /assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png
render_with_liquid: false
---

### 【筆記】Camera

這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。

眾所周知，電腦圖學，尤其是OpenGL實作的相機都是使用pinhole camera\(針孔相機\)，這在很多書都查的到，但是第一個問題是為甚麼採用pinhole，或者說pinhole的優點、缺點是什麼？


![](/assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png)


這邊稍微推薦一下台大VFX跟清大的電腦視覺開放式課程，裡面都有講到。

[**Digital Visual Effects**](https://www.csie.ntu.edu.tw/~cyy/courses/vfx/20spring/news/){:target="_blank"} 
[_With the help of digital technology, visual effects have been widely used in film production lately\. For example, up to…_ www\.csie\.ntu\.edu\.tw](https://www.csie.ntu.edu.tw/~cyy/courses/vfx/20spring/news/){:target="_blank"}


[![](http://ocw.nthu.edu.tw:8888/ocw/upload/teacher/92/92.jpg)](https://ocw.nthu.edu.tw/ocw/index.php?page=course&cid=125){:target="_blank"}


首先，為什麼現實中的相機不使用針孔成像而採用透鏡成像，因為針孔成像需要的曝光時間很久，所以我透過透鏡來聚焦光線進而縮短曝光時間，但是只要有聚焦，那就會產生depth of focus\(DoF\) \(景深\)，也就是在focal plane以外的地方都會變模糊，也就會產生近景、遠景變得模糊。反過來說，pinhole camera就不會有這種問題，他只會有曝光時間很久的問題，而這個問題在電腦中是不存在的，因為我們可以直接把物體上的顏色投影到pixel上，所以我們可以看到大部分的camera都是採用pinhole作為model。

值得一提的是，你可以發現OpenGL或是其他real\-time的rendering都沒有景深就是這個原因，甚至是ray tracing的rendering也有可能沒有景深，所以為了模擬現實的camera照出來的效果，我們會特別修改model去產生景深甚至石直接在後處理的時候hack出景深。

[**Ray Tracing in One Weekend Series**](https://raytracing.github.io/){:target="_blank"} 
[_The Ray Tracing in One Weekend series of books are now available to the public for free online\. They are now released…_ raytracing\.github\.io](https://raytracing.github.io/){:target="_blank"}

pinhole成像的圖片有個特點，就是perspective projection，這個在 [電腦圖學01\-Transformation](https://medium.com/maochinn/%E9%9B%BB%E8%85%A6%E5%9C%96%E5%AD%B801-transformation-%E6%96%BD%E5%B7%A5%E4%B8%AD-ea46dedf01f9?source=---------10-----------------------){:target="_blank"} 有講過，總之就是近大遠小的效果。

以OpenGL來舉例，我們可以複習一下


![](/assets/dee562610e71/0*8Eqies-TqIZASvhg.png)


簡單來說就是經過model\-&gt;world\-&gt;view\-&gt;clip space\-&gt;image space，但值得注意的是clip space\-&gt;image space這段是OpenGL幫我們處理掉的，所以我們並不知道他做了甚麼，那這邊會產生一個疑問，為甚麼要多一個clip space，難道不能從view space\-&gt;image space？

答案是肯定的，其實有另一種流派的perspective projection，甚至比GL的還常見，他的流程就是world\-&gt;view\-&gt;image，甚至是直接是world\-&gt;image。

使用這種transformation一個有名的例子就是OpenCV


![](/assets/dee562610e71/0*5M6AuZdYLOAX4H4W.png)


造成這種差異是因為在電腦視覺中，往往我們就只會有photo以及相機的參數，而在電腦圖學中我們有的是整個場景但沒有image，換句話說，computer graphics 處理的是3D\-&gt;2D的過程，而電腦視覺處理的是2D\-&gt;3D的過程，我們不需要做所謂的depth testing, alpha testing，所以也就不需要NDC或是其他space，只需要單純從world\-&gt;image就行了。

接下來借用下面這篇來說明，詳細還請自己看原文

[**Converting OpenCV cameras to OpenGL cameras\.**](https://amytabb.com/ts/2019_06_28/){:target="_blank"} 
[_Back to Tips and Tricks Table of Contents This post will cover the following scenario: you have the internal and…_ amytabb\.com](https://amytabb.com/ts/2019_06_28/){:target="_blank"}
### OpenCV Camera\(Calibrated Cameras\)


![](/assets/dee562610e71/0*5Do78J-f-uzNMlbQ.png)


簡單來說，就是透過extrinsic matrix\(外部矩陣\) and intrinsic matrix\(內部矩陣\)來從world\-&gt;view\-&gt;image。


![](/assets/dee562610e71/1*7-nq3AUpvuwoxZSEHfvlOg.png)

#### Extrinsic Matrix

簡單來說，就是view matrix，也就是說他也是由一個rotation \+ translation，只是通常GL是4\*4，CV則是用3\*4，也就是到view space的時候只剩3維向量。
#### Intrinsic Matrix

他會直接從view\-&gt;image，這也是一個3\*3的矩陣，所以轉換完的是3維向量，但是最後一個向量是homogeneous，所以其實就是2維向量，也就是pixel的位置。intrinsic matrix裡面的參數就是focal length與principle point之類的東西，focal length\(焦距\)原則上就跟field of view\(FoV\)是等價的東西，principle point則是看你的焦點在image plane上的位置，這邊詳細就不多講了。


![](/assets/dee562610e71/0*BzGgZ09fyzxoEeLu.jpg)


那為甚麼一個叫做內部一個叫做外部呢？因為像是focal length或是principle point都是相機本身內部的結構造成的，而相機的位置以及朝向則是相對來講是外部的參數，換句話說，為甚麼要切成這兩種矩陣，就是因為在大多數情況下這兩者是獨立控制的。

從這之後我會稱這種系統的camera為CV camera，這樣會比較統一。
### OpenGL Camera

接下來我們來複習一下GL的camera，並且觀察一下它與CV的不同。


![](/assets/dee562610e71/0*axI33BTfEpM-kzF0.png)


整個流程就是model\-&gt;world\-&gt;camera\(view\) \-&gt;NDC\(clip\) \-&gt;image。


![](/assets/dee562610e71/1*bS5eecf9hwl1KP931ONhAQ.png)


這邊詳細一樣就不贅述，但是這邊可以注意到它並不是直接從view\-&gt;NDC，而是用了兩個矩陣，這是因為我們在GL常用的perspective projection其實包含兩個部分。

也就是先把視錐轉成長方形，然後在從長方形轉到正方形\(NDC\)，而前者的轉換類似於內部矩陣，而後者其實就是平行投影矩陣。也就是說我們常常使用的gluPerspective之所以不會直接拿來解釋而是直接照填就是因為它還蠻複雜的。

那最後還有一個問題就是從NDC\-&gt;image的這段到底是甚麼，因為通常在GL的pipeline他都會幫你處理好，但是現在我們必須要知道整個轉換的過程才有辦法比較。


![](/assets/dee562610e71/1*C52r-CdvxJ1imv86AizJcQ.png)


這是其中一種做法，總之，我們會需要多一個矩陣將東西從NDC轉換到image。那我們可以稍微比較一下GL與CV的差異，簡單來說就是GL多轉換到了NDC，那為甚麼要這麼做就是因為在NDC我們可以很容易的做許多testing跟一些操作，還有限制值域到一個固定的範圍，而轉換到NDC的成本很低，只需要多幾個4\*4矩陣運算。

概念上，我們可以把GL除了view matrix以外的東西全部乘起來當作是intrinsic matrix，那在最後，我會稍微解釋一下要怎麼用兩種表示方式但能做出一樣的轉換。
### Blender Camera

這邊算是補充一下，原則上blender camera的定義跟OpenGL一樣。


![](/assets/dee562610e71/1*pf02noV322TdnNUtAUB0BA.png)


然後可以注意到Blender的世界坐標系是Z軸朝上。


![](/assets/dee562610e71/1*7n4aaLfMyYWpvXKk4QnsRQ.png)

### Pytorch3D Camera

這個是pytorch3D的camera定義，比較大的差別是\+Z是相機的朝向，在GL是\-Z為相機的朝向。然後在image space上則是類似於window space\(視窗空間\)，左上角為原點，這個跟GL和CV不同。


![](/assets/dee562610e71/0*-iRa5RM30DuQLQ9s.png)


這邊有趣的地方就是Pytorch3D有兩種Camera。


> FoVPerspectiveCameras 





> PerspectiveCameras 





或者你可以在code裡面看到他們的舊稱


> OpenGLPerspectiveCameras 





> SfMPerspectiveCameras 





這樣就一目了然了，前者是類似於GL的camera定義方式，後者則是類似於CV的定義方式，這邊的sfm表示structure from motion，也就是從一連串照片想辦法計算出每張照片的參數，而這些參數通常就是內部、外部參數，所以可以進一步計算出類似於CV camera的內部矩陣、外部矩陣。

我們可以來看看他們的參數設定


[![](https://opengraph.githubassets.com/cd79cb3b16f6c64e9decdd248313d412ce8eaf31f2f6e7f5684a89576004a24c/facebookresearch/pytorch3d)](https://github.com/facebookresearch/pytorch3d/blob/340662e98e97c5e105cf6570765d7bae3e6228bf/pytorch3d/renderer/cameras.py#L79){:target="_blank"}

#### FoVPerspectiveCameras

這邊如果有學過GL應該就會知道參數要怎麼填，原則上就是要填view matrix跟perspective projection matrix。只是這邊可以注意到，K就是view\-&gt;NDC的矩陣，所以如果我們能夠預先算出來的話就不用填znear, zfar, fov, aspective ratio，用GL的話說就是我們如果自己算好projection matrix我們就直接glLoadMatrix，不需要再用gluPerspective。


![](/assets/dee562610e71/1*5ZWKbum7EEibCsGn9RjESA.png)


然後這邊的R跟T就是view matrix的rotation matrix以及translate matrix，這邊要提醒一下，view matrix是world\-&gt;view，所以T並不是camera在world space的位置，但如果做個反矩陣就變成view\-&gt;world，那此時的T就是camera在world space的位置。


![](/assets/dee562610e71/1*aGg8ZhAfTaIxGp2lk1sYHA.png)


最後我們可以來看一下K的計算方式，原則上對應的就是OpenGL perspective projection matrix的填法，只有兩點要注意，他這邊的fov是vertical fov，也就是垂直軸的視角，不要填成水平軸的視角除非你是正方形，那就沒差；第二個是在\[3\] \[2\]的位置的值是\+1，而GL是\-1，這是因為GL是\-Z朝前，Pytorch3D則是\+Z朝前。
#### PerspectiveCameras

這邊在外部參數的部分跟FoVPerspectiveCameras是一樣的，就是R跟T。


![](/assets/dee562610e71/1*qjtQRI6VvkKyWzNOqXaTKg.png)


那唯一的不同就是K矩陣的定義，這邊要注意的是，一般來說CV camera的K是view\-&gt;image，但是在pytorch3D我們仍然需要做一些testing，所以這邊的K是view\-&gt;image。


![](/assets/dee562610e71/1*R3dFw87kkAtYxbkk0-UDfg.png)


這邊的focal length有兩種單位，一個正常的單位，另一個是以pixel為單位，舉例來說，我們可以設定focal length為1mm或是1px，如果是後者的話你必須在給你的image resolution。


![](/assets/dee562610e71/1*Ru2VMGxyB68RM7aNp_eSjg.png)


這邊我自己有個小疑問就是我們要怎麼將focal length轉換成fov，原則上單獨的focal length並不能直接轉換成fov，你必須要有sensor size，這跟image resolution並不一樣。


![](/assets/dee562610e71/0*5rDgMypztItqAcnA)


以真實的相機來說，我們的成像平面叫做sensor，然後這個sensor size是一個定值在一個相機裡面，所以我們可以透過調整focal length來調整視角，但是如果只有focal length並不能決定fov，你可以想像sensor size越大，這個fov就會越大。

而image resolution跟sensor size原則上是獨立的，假設我的sensor size是10\*10mm，但我的resolution可以是800\*800, 也可以是1920\*1080，這只是差在我們怎麼去分割這10\*10mm的sensor，所以可以算出來每個pixel的大小，例如前者每個piexl的大小是\(10/800, 10/800\)mm，後者則是\(10/1920, 10/1080\)，這可以發現，pixel不一定是正方的，有可能是長方的。

但是你可能會有疑問，為什麼CV camera的K裡面並沒有sensor size的參數，但是GL camera需要有完整的FOV資訊呢?我們可以先用簡單的case來說明。


![](/assets/dee562610e71/0*QWZtUsqn43Hnqxmz)


假設我們的focal length為0\.5mm，我們原點就在圖片中心，所以priciple point為\(0, 0\)，那麼我們K就變成一個單純的scaling matrix，所以如果我們的在view space有一個點\(0, 1, 1\)那就會投影到\(0, 0\.5\)的sensor上，\(0, \-0\.5\) \-&gt;\(0, \-0\.25\)，\(圖上為了示意所以負的在上面，正的在下面\)，但是我們的sensor size是0\.6mm，所以超出的部分就直接丟掉。

所以我們不需要在K裡面有sensor size的資訊，因為我們會直接丟掉。但在GL的projection我們要把sensor size\(例如上面的\+0\.3~\-0\.3\)塞到NDC\( \+1~\-1\)，所以一定要有sensor size的資訊，所以才需要FOV。那我們可以在看一個比較實際的例子。


![](/assets/dee562610e71/0*4sBcaeGz39vef-ns)


假設我們的image的resolution是1920\*1920px，那我們的focal length則是100px，那我們也可以將點對應到image plane上，所以只要我們確保focal length跟sensor size的單位是一致的就好了。

所以回到PerspectiveCameras的K，我們可以給image size如果我們的focal length的單位是px，那他預設的sensor size是\[ \-1, \+1\]，principle point是\(0, 0\)，也就是從view space\-&gt;NDC，所以如果我們有fov也可以透過上面例子的那種方式反推我們要的focal length，實際上要怎麼計算我們下一節再介紹。


[![](https://pytorch3d.org/img/pytorch3dlogoicon.svg)](https://pytorch3d.org/docs/cameras){:target="_blank"}

### Camera: Blender to Pytorch3D

為了從Blender的camera輸出到Pytorch3D，我們要先來搞懂Pytorch3D的Camera要怎麼使用，甚麼?上面不是介紹過了?這邊還有很多坑\. \. \.，如果想跳過直接到Blender to Pytorch3D可以直接滑到下面。


[![](https://colab.research.google.com/img/colab_favicon_256px.png)](https://colab.research.google.com/drive/1ZBlX1O5kTDyei-hJ9xuVKGel_mv8i8QA?usp=sharing){:target="_blank"}


這邊可以自己跑跑看上面的script，原則上就是拿Pytorch3D的教學來改，接下來會接著說明。
#### Install and Import modules

這邊就是安裝Pytorch3D，就照官方的走就好了。
#### Load a mesh and texture file

這邊也是官方的牛牛，就照著走就好了。
#### Create a renderer

這邊要建renderer，我們可以用LookAt的工具來產生rotation matrix跟translation matrix。這邊要注意Pytorch3D是採用row\-major，可以在文件看到。


![](/assets/dee562610e71/1*Cf9p0DbK792RwAx-K2kQXg.png)


[**Geometry \(Row Major vs Column Major Vector\)**](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/row-major-vs-column-major-vector){:target="_blank"} 
[_Earlier in this lesson, we have explained that vectors \(or points\) can be written down as \[1x3\] matrices \(one row…_ www\.scratchapixel\.com](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/row-major-vs-column-major-vector){:target="_blank"}


![](/assets/dee562610e71/1*Wk68NtmIJ8-nQryb_22uog.png)


這邊可以看到，這邊的\[R\|T\]是\(world to\)view matrix，所以這邊的T的值並不是camera position in world space，而是world position in view space。以上圖微例，我們讓相機朝向\(0, 0, 0\)，而相機與其的距離為2\.7，所以我們的camera實際上的位置是再\(0, 0, \-2\.7\)，如果要驗證的話可以拿\[R\|T\]去做inverse，因為R是identity，所以我們直接把T加個負號就是inverse，也就是\(0, 0, \-2\.7\)，這就是camera position in world space。


![](/assets/dee562610e71/1*9j7_mYSO0FauoEMKgUtMEQ.png)


我們透過RT就可以把fov camera建出來，這邊可以注意到projection matrix的1\.7321其實就是focal length，順帶一提這邊預設的fov是60度。


![](/assets/dee562610e71/1*0gcbAvsdzy2j9bCUmlERDw.png)


sfm也可以用類似的方式建出來，這邊預設的focal length是1。


![](/assets/dee562610e71/1*K6pycHMOQyU6q82U0Sk9vw.png)



![FoV\(left\), SfM\(right\)](/assets/dee562610e71/1*zqUF_z-eDhRFutVrOiSuPg.png)

FoV\(left\), SfM\(right\)

這邊可以發現fov camera的視角比較小，我們可以看到sfm camera的視角比較大，我們可以直接看兩者的projection matrix就可以發現前者的focal length比後者大。
#### Rendering by FoV Camera or SfM Camera

那如果我們想要用兩種camera能夠render出相同的圖片，那我們就要改一下fov或是focal length。


![](/assets/dee562610e71/1*--KwFE-0Tq4DahpiBN5XVQ.png)


這邊其實可以依據前面所講的可以把fov轉成focal length。


![](/assets/dee562610e71/1*eciG6g4ueYajAZBKND8bAg.png)


或是用更暴力的方法，直接拿fov的projection matrix丟進去，可是這邊就有一個疑問，他們的矩陣除了focal length一樣，其他都不一樣，這樣OK嗎?
#### Projection and Unprojection

Pytorch3D提供了projection跟unprojection，前者表示從world space to NDC space，後者則是相反。首先我們可以先看fov camera


![](/assets/dee562610e71/1*PyBN0V6ISlm7NvTPqxPXbA.png)


可以看到如果我們在NDC\(0, 0, 0\)做unproject到world space就是相機的位置world\(0, 0, \-2\.7\)，NDC\(0, 0, 1\)也就是在相機前面1單位\(in NDC\)，我們可以得到world\(0, 0, \-1\.7\)，其實這邊就有點奇怪了，照理來講通常NDC的一單位會大於world space的一單位，但是我們看下去\. \. \.

如果我們從world\(0, 0, 100\) transform 到NDC\(0, 0, 1\)，就大約是在NDC的最遠了\(z far\)，然後world\(0, 0, 0\) \-&gt;NDC\(0, 0, 0\.636\)，world\(0, 0, \-1\.7\) \-&gt;NDC\(0, 0, 0\)，這邊可以發現，距離相機越遠，在NDC裡的值越大，這非常合理，然後為甚麼world\(0, 0, \-1\.7\)會直接轉換到NDC的原點呢?因為我們znear=1，所以NDC的原點就是camera位置在往前1單位，所以world\(0, 0, \-2\.7\) \-&gt;NDC\(0, 0, \-0\.432\)，拿相機原點去做反而會變成負值。

這邊另一個有趣的點是，NDC\(0, 0, 1\) \-&gt;world\(0, 0, \-1\.7\)，但是world\(0, 0, \-1\.7\) \-&gt;NDC\(0, 0, 0\)，這就表示這兩者的轉換並不是inverse的關係。這個我們可以在另一段code看到一些蛛絲馬跡。


![](/assets/dee562610e71/1*2RFIevxsDW0SNjYvSf4zZw.png)


這是MeshRasterizer的transform這邊很明顯的可以看到他直接把算出來的z值拿view space的z值蓋掉。

那我們在來看看相同的sfm camera。


![](/assets/dee562610e71/1*MZ6ZoswO2Z5hytK3mUTbfQ.png)


這邊sfm並沒有znear，但是在NDC\-&gt;world上跟fov camera相同，但是在world\-&gt;NDC就不一樣了，可以看到距離越遠值反而越小，意思就是越遠的東西深度值越小，這非常反直覺，但是我們知道後來的z值會被直接蓋掉，所以其實這邊的z值並不是很重要。

所以這邊也回答了前面的問題，為什麼可以直接拿fov camera的K給sfm camera，因為他們差異的地方只會造成z值的變化，但因為之後都會被直接蓋掉，所以沒差\(乾，害我想很久\)。
#### Blender to Pytorch3D

終於可以回到正題，先直接看看腳本吧。


![](/assets/dee562610e71/1*k0XkJoj3tO9j_0Ad7yhqgw.png)


首先，因為Pytorch3D的camera的參數就是R, T, K，所以這邊就要想辦法輸出內部、外部參數。

首先是外部參數，其實我們就是要算出pytorch3D world space to pytorch3D camera\(view\) space的矩陣就行，詳細之後會解釋，但這邊其實我是計算OpenGL world space to Pytorch3D camera space，因為GL world跟Pytorch3D world都是y朝上，所以我就直接用了，只是camera跟mesh都有轉到一樣的坐標系其實都可以。

那內部參數就是記得要拿vertical fov，因為pytorch的fov是vertical fov。


![](/assets/dee562610e71/1*WIQnxfhdFJULVw-QNIRiqw.png)


這邊要記得sensor\_fit要調成vertical他才會給你正確的vertical fov，然後我們可以計算出需要的focal length，然後把需要的矩陣都填出來，然後輸出就行了。
