---
title: "【工作】NVIDIA GTC Taipai 2025 出差紀錄"
author: "帽捲"
date: 2025-09-24T16:43:54.091+0000
last_modified_at: 2025-09-28T14:04:21.427+0000
categories: ["Maochinn"]
tags: ["gtc-2025","physical-ai","digital-twin","omniverse","cosmos"]
description: "從年初的在美國GTC，沒想到五月的時候台北也有一場，這次雖然算是出差，但是因為不好說的理由，這次是我自己自費的，這個活動去年的名稱好像是AI summit，今年你可以當作是在Computex旁邊，NV開一個自己的主場的概念"
image:
  path: /assets/4733ef5de7c8/1*gD2ip6cR_WqUuRt3oW1Sgg.png
render_with_liquid: false
---

### 【工作】NVIDIA GTC Taipai 2025 出差紀錄

從年初的在美國GTC，沒想到五月的時候台北也有一場，這次雖然算是出差，但是因為不好說的理由，這次是我自己 **自費** 的，這個活動去年的名稱好像是AI summit，今年你可以當作是在Computex旁邊，NV開一個自己的主場的概念


![[https://www\.nvidia\.com/zh\-tw/gtc/](https://www.nvidia.com/zh-tw/gtc/){:target="_blank"}](/assets/4733ef5de7c8/1*gD2ip6cR_WqUuRt3oW1Sgg.png)

[https://www\.nvidia\.com/zh\-tw/gtc/](https://www.nvidia.com/zh-tw/gtc/){:target="_blank"}

活動嚴格上來看只有兩天，但是在5月20日跟5月22日各有一個全天workshop，所以可以看成是三天，且報名了workshop，同時也就可以參加每一天的session。


![[https://www\.nvidia\.com/zh\-tw/gtc/pricing/](https://www.nvidia.com/zh-tw/gtc/pricing/){:target="_blank"}](/assets/4733ef5de7c8/1*KNNMjrl-ComfyhZ3Qj7YEg.png)

[https://www\.nvidia\.com/zh\-tw/gtc/pricing/](https://www.nvidia.com/zh-tw/gtc/pricing/){:target="_blank"}

workshop又分成兩個，一個就是Physical AI，一個是LLM的，那身為這幾個月都在Omniverse, Isaac和cosmos等NVIDIA的東西中打滾，實在是沒有理由不去報名了，


![[https://www\.nvidia\.com/zh\-tw/gtc/workshops\-and\-training](https://www.nvidia.com/zh-tw/gtc/workshops-and-training){:target="_blank"}](/assets/4733ef5de7c8/1*8XXBJrn-z-Dn4m54qI3UXg.png)

[https://www\.nvidia\.com/zh\-tw/gtc/workshops\-and\-training](https://www.nvidia.com/zh-tw/gtc/workshops-and-training){:target="_blank"}

因此我就參加了5月20到5月22日全程，並且有繞到旁邊Computex湊個熱鬧，因為後續幾天基本上只要是專業人士AKA有統編就可以入場，基本上session中的廠商大多都在Computex中有攤位。

所以比照 [【工作】SIGGRAPH Asia 2024 東京出差紀錄 / 攻略指南](../bf50ed0bf508/) 的形式，每一天的細節內容會在個別文章中描述，本篇會作為整體的review、Conclusion和心得
### 行後整理

個別活動的心得我就放在個別的文章，這邊就不贅述
- [【工作】NVIDIA GTC Taipai 2025 — Day 0：Building Digital Twins for Physical AI With NVIDIA Omniverse](../6560b300f03c/)
- [【工作】NVIDIA GTC Taipai 2025 — Day 1：Manufacturing sessions](../230d146467b3/)
- [【工作】NVIDIA GTC Taipai 2025 — Day 2：Physical Al and Robotics Sessions](../616f6e63db42/)

### Review

因為我是電腦圖學工程師，近期碰的比較多的是Digital Twin，所以，我比較注重的是Physical AI中的幾個部分
- NVIDIA Omniverse
- Sim2Real Gap
- Synthetic Data Generation
- NVIDIA Cosmos
- VLM


講白話一些的話，依據 [Day0的開場介紹](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#dbae){:target="_blank"} ，大概就知道整個架構分成三塊，要訓練出機器人大概有三個部分，一個是模擬，第二是訓練，第三是部屬，其中模擬與我的關係最大，因為模擬的好壞會實際影響訓練的好壞，而一種實際的應用就是在虛擬環境中模擬生成合成資料\(Synthetic Data\)，以此做為訓練資料，而在本次活動，NV提出來的實際方案就是Omniverse with Cosmos


> NVIDIA Omniverse 





Omniverse是一個平台，要解決的是不同3D軟體之間的複雜協作問題，或者說，他們希望不同的DCC之間可以用openusd作為檔案格式做溝通，而這個實際上的實作就是omniverse，這可以從我在 [Day0 Workshop的QA](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#e151){:target="_blank"} 中推敲出來。

另外，在 [Day1](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#28b6){:target="_blank"} 的各種議程中發現，為什麼大多數的應用都是機器人或是大型的工廠，這就是因為這些應用本身夠複雜，需要多個軟體之間的協作才能完成，需要建模、模擬、同步等，此時使用omniverse才有價值。反過來說，如果本來就已經串接好，足夠成熟或是足夠簡單的應用，如 [Day1的Fii](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#881b){:target="_blank"} 或是 [TSMC](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#46d0){:target="_blank"} 。後者也把話講得很白，目前的模擬很難真的全部串接起來，這也與技術的成熟有關，產業上還是求穩再求好。

另外也有提到，模擬有時候是不得不的方式，因為現實的資料難以取得，只能用模擬代替，但此時，模擬的是否準確就是一個難題，另一個好處是Day2的Solomon提到的，模擬也有助於跟客戶溝通。

理想上，任何事情都可以先用Digital Twin來模擬Physical AI，這聽起來十分美好，但這個概念是幾十年前就有了，因為有Sim2Geal gap這個問題。


> Sim2Real Gap 





簡單來說，因為模擬與現實的差距勢必有差距，這個差距該怎麼彌平，或者說跨越是個大哉問，這個問題也是我自己過去經驗上，現在實務上遇到最直接的問題。

解決這個問題的其中一個常見的切入角度是，我們需要建構sim\-ready asset，這也是實務上遇到最大的瓶頸，具體來說，我們除了要重建對應的3D mesh這種幾何的參數之外，還要再加上各種物理參數、關節點等參數才會構成sim\-ready asset，更實際的問題是要附加多少參數？如何設定？

前者根據 [Day0 Omniverse講師的答覆](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#93af){:target="_blank"} ，他們是希望由提供asset的廠商要提供sim\-ready asset，不能只有3D model，也就是由製造的人自己決定要附加多少參數。而 [Day1 Wistron](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#28b6){:target="_blank"} 則表示，需要domain expert，他們才知道有甚麼關鍵參數需要被模擬，否則就如 [Day1的TSMC](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#46d0){:target="_blank"} 所述，如果沒有足夠準確的真實、關鍵資料做為模擬的依據，所謂的assets不過就只是3D model。

後者在和 [Day 2 MetAI](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#6230){:target="_blank"} 的問答中知道，這種調整現階段還是迭代的去調整，沒辦法一步到位，因為現實與虛擬的計算模型並不一致，勢必還是有調整參數的環節，因此 [Day1 Delta](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#7ac1){:target="_blank"} 將重點放在如何輔助迭代的更加自動且快速，在 [Day 2的專家交流會議](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#b5eb){:target="_blank"} 上，Physical AI的專家也表示，手動調整參數在現階段還是最有效的方式。


> Synthetic Data Generation 





但針對sim2real gap，並不是把虛擬與現實做的完全相同這種做法，因為我們不可能將現實中所有的參數都能夠完美的重建以及同步， [Day1與cosmos團隊討論](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#d0e1){:target="_blank"} 中發現，他們認為的gap是來自於資料量不足，而不是資料不夠保真，換言之，解決gap的手段不只是讓asset更像現實，如何使用asset來產生資料，產生甚麼樣的資料也是一種方式，這其實就是Synthetic Data Generation。

比較典型的說法是，如 [Day2 Solomon](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#be9a){:target="_blank"} ，合成資料可以有效地補充資料的不足，這是因為現在的model有許多非常龐大，這是因為這些LM是要解決複雜的問題，於此同時，訓練的資料也需要更大、更多樣以及更好，因為我們希望這些LM也可以學到更加底層、核心的邏輯，例如LLM從大量的接龍中學到邏輯。

就像 [Day1 Pegatron](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#a929){:target="_blank"} 提到的，過去常見的perception AI我們都是一味的追求準確度，但是現在的model我們其實更在乎他能不能真的理解，並且自動的去完成任務，而不只是看某個環節的準確度。如果LM可以變成agent，他真的能夠理解問題，那他甚至可以自己生成synthetic data，自己調整參數，達到self\-improving和scalable，真正做到自主化。

換言之，我們不一定要讓合成資料完美的像現實的資料，而是能學到我們想讓AI學到的底層邏輯，對任務的層級來說，我們就已經克服了Sim2Real Gap，因為我們並沒有複製一個真實世界出來。

但這一切也起來非常美好，但是實際上的SDG的方式大多還是停留在使用rendering去產生圖片的方式，例如使用Blender, Omniverse RTX之類的，其實際的成效有限， [Day 2的專家交流會議](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#b5eb){:target="_blank"} 中提到，目前實務上為了要貼近現實，基本上還是得仰賴CG專家的補償，另一方面， [Day1與cosmos團隊討論](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#d0e1){:target="_blank"} 則是從特定抽象特徵的多樣性去著手，讓合成資料可以克服sim2real gap。


> NVIDIA Cosmos 





那具體來說，Cosmos到底是甚麼，如果從 [【Cosmos】Paper Reading: Cosmos World Foundation Model Platform for Physical AI](../a6b5ff405748/) 中提到，一種只看輸入與輸出的概念來思考， [Cosmos就只是一個生成影片的AI](https://medium.com/maochinn/cosmos-paper-reading-cosmos-world-foundation-model-platform-for-physical-ai-a6b5ff405748#ffa3){:target="_blank"} ，這個解釋雖然過於粗暴，但是可以有一個比較直觀的認知。

重點在於，這提供了另一個思路去生成合成資料，一種有別於人為設計模擬的方式去生成，例如Omniverse，而是可以直接使用prompt這種抽象指令的方式去生成合成資料。
- 前者的問題是，我們常常只能透過調整底層的參數，例如摩擦係數的方式去生成不同情境的合成資料，但這樣的調整非常地不直觀如隔靴抓癢，需要大量的嘗試與專家的介入，尤其是你追求的抽象特徵的多樣性時。
- 後者的方式就是透過文字、圖片的prompt直接讓AI去生成對應情境的合成資料，當然，這種方式的風險就是合成資料本身可能不符合一些物理現象，因此才誕生出了cosmos這樣的平台，旨在訓練出world model，這些model都是理解物理規則的。


以此對應的一些實際的world model例如， [Day 0介紹到Cosmos\-Reason1](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#79b8){:target="_blank"} 有提到他是用來檢查影片是否有不符合物理的地方，這點呼應了keynote的 [GR00T\-Dreams](https://github.com/NVIDIA/GR00T-Dreams){:target="_blank"} 流程，因為Cosmos生成出來的資料目前仍不完美，可能會有物理上不合理的部分，例如奇怪的穿模之類的，用 [Day2 Foxlink](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#bf75){:target="_blank"} 的話來說，這些world model在預測未來的時候就是在 **做夢** ，而我們必須從中取出好的美夢出來，換言之，他們也很清楚cosmos著重點在於多樣性，儘管他們號稱它能夠是物理上準確的，但是還是需要一個機制去過濾掉不符合物理性質的。

換言之，這可以延伸一個議題【World Model】，world model的輸出終究是一個主觀，它不是將現實世界完整的、客觀的複製到電腦，而是以某人的視角，將他所觀察的世界，該如何理解的邏輯，encode成world model，所以輸出的結果是這個人對於未來的預測。

另外，有趣的是，在論文中 [【Cosmos】Paper Reading: Cosmos World Foundation Model Platform for Physical AI](../a6b5ff405748/) 中有提到，評估表現的時候是是拿物理跟渲染引擎去生成符合物理定律的模擬影片，而在 [Day1與cosmos團隊討論](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#d0e1){:target="_blank"} 的時候發現其實就是用omniverse的physX去做模擬。

另一方面，world model的硬體成本不容忽視，具體來說， [Day0 workshop的時候](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#9e6e){:target="_blank"} 是開jupyterLab，裡面用兩張H100，只是用來做cosmos的inference，換言之，你要finetune甚至於train一個自己的WFM，沒有一櫃H100應該是不用想。

總之，cosmos是為了解決資料量不足的問題，具體來說，就是要解決機器人的訓練資料不足的問題，更準確來說，Physical AI中VLM抽象語意的訓練資料不足。


> VLM 





Physical AI比較典型的例子就是機器人，從 [Day1 NVIDIA](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#0268){:target="_blank"} 的說明中就可以知道他們想要打造機器人的ecosystem，因為目前還沒有一個確定的solution，都是拿現有的軟體拼拼湊湊，因此NV想要推動以omniverse為平台來發展基於NV硬體的ecosystem，實際的案例可以參考 [Day2的Foxlink](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#bf75){:target="_blank"} ，

事實上，如果光看現有的應用，可能會覺得這些應用顯得有點硬要，如果用傳統的自動化設備可以做得更快更好，但機器人的優勢在於更有彈性，如同 [Day1 Fii](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#881b){:target="_blank"} 所述，同一個機器人可以負責不同的工作，甚至可以沿用到下一年度的生產，中間可能只需要經過AI的finetune即可，換言之，如同 [Day2 Solomon](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#be9a){:target="_blank"} 所述，機器人不再依賴特定的環境，甚至能夠解決過去難以用規則描述的複雜問題，我們希望AI有VLM能足夠聰明，真正理解畫面，並且自己去動態的去調整自己的策略。

這個概念在 [Day2 研華、ADATA](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#890a){:target="_blank"} 也有提到，如果AMR能夠有VLM真正理解畫面，那就可以應對動態複雜的環境，例如醫院，我們可以透過事先大量不同的環境作為訓練資料，來訓練AI理解底層的核心邏輯，而不是依賴在某個已經設定好的環境下，用拆解的簡單規則作為核心邏輯。

這其實就是 [Day2 Solomon](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#be9a){:target="_blank"} 提到VLM時表示，這些synthetic data的目的在於教會model一些底層的邏輯，因此未必需要非常擬真，可能是用在前期的pre\-trained model。當然synthetic data若是要補充現實資料的多樣性，那就需要擬真了，這也就是Day2 metal所說的差異化樣本。

另一方面的好處是，如果機器人真的能夠真正了解畫面的意義，那就能夠更直接且直覺的教導機器人動作，例如 [Day2 Foxlink](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#bf75){:target="_blank"} 和 [Solomon](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#be9a){:target="_blank"} ，都會結合使用vision pro來做為訓練資料，這同時也是因為機器人雖未必要仿人的雙手與雙足，但是若與人有類似的結構，那人類就可以Imitation Learning。

除了機器人之外，另一個適用VLM去監視人員，例如， [Day0 Workshop最後部分](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#43b0){:target="_blank"} 的時候展示了SOP agent，另一個就是 [Day1 Wistron](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-1-manufacturing-sessions-230d146467b3#28b6){:target="_blank"} 提到的，總之這些VLM使用合成資料的Model都有個特點，就是很龐大，換言之，訓練資料的不足得依靠合成資料來克服。
### 結論

經過三天的GTC Taipei 2025活動，從Day 0的workshop到Day 1的製造業sessions，再到Day 2的Physical AI和機器人sessions，我對NVIDIA在Physical AI領域的布局有了更深入的理解。
#### 核心洞察：從合成資料到Physical AI的演進

這次活動讓我重新思考了合成資料這個概念。過去在研究所時期，我們稱之為「假資料」，用遊戲引擎或簡單規則生成訓練資料，但往往遇到sim2real gap的問題。現在這個概念重新被熱烈討論，關鍵差異在於兩個方面：

第一，模型變得更加龐大且聰明。現在的pre\-train model概念更接近LLM，不是針對特定任務優化，而是希望模型學會底層的核心邏輯。就像LLM從大量接龍中學到邏輯推理能力，Physical AI也需要從大量合成資料中學到物理世界的底層規律。

第二，合成資料的品質大幅提升。現在的diffusion model能夠產生photo\-realistic的畫面，品質與多樣性都有長足進步。更重要的是，我們不再需要完美複製現實世界，而是讓AI學會我們希望它學會的底層邏輯。
#### NVIDIA的策略：平台思維

從這次活動可以看出，NVIDIA的策略是提供平台而非解決方案。Omniverse作為平台，要解決的是不同3D軟體間的複雜協作問題，透過OpenUSD作為共通語言，讓各種DCC能夠串接。Cosmos則是為了解決訓練資料不足的問題，特別是Physical AI中VLM所需的抽象語意資料。

這種策略的優點是能夠快速建立生態系，但缺點也很明顯：NVIDIA只負責讓流程可行，不負責把每個環節做好。實際使用中遇到的bug和問題，需要靠個別軟體廠商或使用者自己解決。
#### Sim2Real Gap的重新定義

與專家對談後發現，Cosmos團隊認為sim2real gap主要來自資料量不足，而非資料不夠保真。這改變了我們對合成資料的認知：重點不在於讓合成資料完美像現實，而在於產生足夠多樣且有效的訓練資料，讓AI學會底層邏輯。

這種思維的轉變很重要。我們不需要複製一個完美的真實世界，而是要讓AI在虛擬世界中學會處理現實世界問題的能力。這正是Cosmos的價值所在：透過world model生成多樣的合成資料，讓Physical AI能夠應對各種複雜情境。
#### Physical AI的未來

Physical AI的核心是讓機器人能夠真正理解環境並自主決策。這需要大量的訓練資料，而現實資料往往難以取得或成本過高。合成資料的生成因此變得關鍵，但關鍵不在於擬真度，而在於多樣性和語意豐富度。

從製造業的案例可以看出，Physical AI最適合解決那些難以用簡單邏輯拆解的複雜問題，如機器人在動態環境中的操作。對於已經成熟的封閉問題，傳統的rule\-based方法可能更適合。
#### 技術與商業的平衡

NVIDIA作為硬體公司，最終目標還是要賣自家的晶片。因此他們會不斷推出新的流程圖和工具，但重點是讓這些工具能夠在NVIDIA的硬體上運行。這種策略有其合理性，但也意味著使用者需要承擔更多的整合和優化工作。

總的來說，這次GTC Taipei 2025讓我看到了Physical AI領域的巨大潛力，也理解了技術發展背後的商業邏輯。雖然還有許多技術挑戰需要克服，但這個方向無疑是正確的，值得持續關注和投入。
### 心得

從今年年初GTC丟出Physical AI，以及Cosmos等東西，大概就知道NVIDIA是很想要推這些東西，當時我從三個scaling law發現它背後就是要推三種自家的機器，分別是OVX、DGX和AGX，因為他們畢竟是賣硬體的公司，所以要想辦法基於自己的硬體之上想出吸引人的應用，當然也不只是硬體，還有lib。

但直到這次參加完，我有一個更具體的感受，講難聽一點，NV為了要賣自家的硬體，在不同的畫各種藍圖和流程圖，但是也在頻繁的修改或是推出新的流程圖，這是因為，對他們來說，無論流程圖長什麼樣，能做什麼東西，只要是用他們的機器，那麼他們的目的就已經達到。

所以可以發現，NV需要各種案例，來說服，或者說吸引更多人相信他們畫的流程圖是可行的，因此

另一方面，因為NV畫的是流程圖，他們要致力於把流程圖中的流程實際上可以執行，因此他們會開發許多lib去把流程中的每個節點做出來，例如omniverse的extension，但也僅僅只是做出來，他們不負責把東西做好，只負責把東西做出來，因此實際上有在使用的人都多多少少遇到bug或是有不太好用的feedback，這就是因為，NV只負責讓整個流程可行，而具體每一步做得好不好不在於他們的考慮範圍，例如，如果有人問到simulation可以做到多少精度，或者是說想要推動公司用omniverse app去取代現有的工具，但往往得到的答覆諸如，Omniverse是平台，沒有要幹掉誰，

這就是因為，NV從omniverse到cosmos或是推出的其他東西，他們都是lib，是專門在NV的顯卡上跑的lib，這些lib可以收集起來就變一個平台，只要在這個平台就可以這個lib接那個lib，那流程圖就可以用這些lib串接起來達成，當然，也可以接受第三方的軟體，因此，NV的想法就是，流程中每個節點都可以用各種軟體來做，但是這些軟體都是run在NV的卡上\(可能是用到NV的lib\)，但如果目前沒有可用的軟體，他們才會自己下來寫軟體把流程接起來，但是效果不好，他們也不太在意，因為這應該是專門的軟體應該要想辦法解決的，它們不負責。

因此，omniverse的目標是把專門模擬的軟體透過openusd接到omniverse app上，同時也有專門建模的軟體、專門渲染的軟體等等，這些軟體都是run在NV的卡上，但是效果不好要去找個別的軟體廠商，他們不負責，現在推出來的軟體只是為了讓流程run起來的可用方案。

另外一些八卦，從 [Day0的講者](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-0-building-digital-twins-for-physical-ai-with-nvidia-omniverse-6560b300f03c#93af){:target="_blank"} 還有 [Day 2的專家交流會議](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-day-2-physical-al-and-robotics-sessions-616f6e63db42#b5eb){:target="_blank"} 再加上與所羅門的員工談論下來，有種感覺是NV中就是多個獨立團隊在運作，彼此的水平合作不多，結果造成相容性蠻差的。而NV對於許多實務的困難看起來都被輕鬆帶過，但實際上應該是很大的問題，一開始覺得怪怪的，但是後來才明白這些問題本來就不是設定要NVIDIA要解決的問題，例如模擬的準確度，asset的建構等。
### 補充思考

這部分記錄一些思考，但與上面的本文較無直接相關
#### 模擬的商業價值

從這次活動中發現，模擬除了技術價值外，還有重要的商業價值。如Solomon提到的，模擬也有助於跟客戶溝通；TSMC則指出，現實的資料難以取得時，模擬成為不得不的選擇。

總結來說，商業上，cosmos有兩個亮點：Photorealistic和多樣性。但在經過跟專家對談後發現，其實核心要解決的是資料不夠的問題，更進一步說，不夠的是更抽象語意的資料，例如機器人的動作，這種訓練資料的獲取成本過高。
#### 合成資料的演進

上一代的假資料要求的就是兩點：快和真。因為需要補足現實資料的不足，所以要夠真，但是因為不能太真，所以會要randomize，因此量要大，既然量要大，那就要夠快。

雖然說garbage in garbage out，但我認為合成資料跟model是一體兩面的，我們要為了生成資料的時候要考慮要讓AI學會什麼，AI也會依據資料學到東西，雖然不見的是我們要它學會的。
#### 技術猜想

Sim2real gap沒有標準答案，但我相信有幾個常見的問題可以整理出來，針對這個痛點去解決。OpenUSD只是生態系的入場券。

如果只考慮幾何，沒有物理性質，這會讓sim2real的gap極具縮小，或許可以從時間上clip，把所有物理現象簡化為Position\-based的線性行為，理論上只要clip無限縮小，就可以無限逼近真實的預測，當然這牽涉到算力也要能在無限小的時間內偵測與預測。
#### 封閉問題vs非封閉問題的深度思考

概念上來說，過去解決問題的方式就是依據人類的知識將一個非封閉的問題拆解成一個又一個封閉問題，數學上是、產業上也是，例如將製造流程拆解成數個封閉的設備各自獨立負責的問題，每台設備在預先設計好的靜態環境下處理即可，這麼做的好處是，每個封閉問題不受彼此干擾，且單一問題簡單，問題組合的邏輯也簡單，例如一條流水線的加法，只要出問題，就可以很直觀的針對單一的封閉問題去處理。

但缺點也很明顯，只要問題超出了原先考慮範圍，在一個動態環境下就會變成非封閉問題，最直觀的解決方式就是重新增加限制或是重新考慮封閉範圍，總之，會再把問題想辦法簡化成另一個封閉問題，在這樣的思路下面，如果我們要的是用大量簡單的邏輯解決複雜問題，其實使用rule\-based方式並無不可，事實上，實際的產線上，仍多是以rule\-based去解決，頂多將部分獨立的問題轉由AI去解決，但本質上，整個問題仍是被拆解成各自獨立的封閉問題，因此，AI自然而然處理的就是封閉環境下的問題。

就像是AI也會追求end\-to\-end的解決方案，如果AI可以參考完整的資料，那我們可以期待AI綜合所有的資料發現一些神奇的邏輯，比起簡單的邏輯可以更有效的解決問題。這樣的概念套用在產線，就是工業4\.0，讓AI可以掌握整條產線甚至是工廠的資料，但這確實有點沒事找事，本來的方案運行的好好的，因此你也可以看到推動的速度很緩慢，因為這就像是你把一堆簡單的問題硬是要去混在一起變成一個複雜的問題來解決。

我們應該要解決的問題應該是那些難以用簡單邏輯拆解的複雜問題，例如機器人，或者說，Physical AI，因為它牽涉到大量的複雜的輸入以及輸出，輸入可能是各種sensor在各種環境下，輸出可能是各種motor在各種環境下，這種非封閉問題我們難以分解成數個封閉問題來解決，因為每個輸入都可能對每個輸出有所影響。

如果我們已經有成熟的解決方案，可以將複雜的任務人為的拆成數個簡單子任務，也就是將一個非封閉的問題拆解成數個封閉的問題，各個獨立解決，或許也就不需要硬是將所有一切都串接起來。

感謝你的閱讀，如果你對我的文章對你有所幫助或是意見歡迎回覆，如果你想要支持我可以：
- 拍個手👋，或是分享一些想法💬
- [追蹤我的頁面](https://medium.com/@maochinn){:target="_blank"}
- [訂閱我的專欄](https://medium.com/maochinn){:target="_blank"}
- [透過LinkedIn聯繫我](https://www.linkedin.com/in/chih-wei-chang-6526801b2/){:target="_blank"}


或是觀看我其他系列文章
- [【工作】NVIDIA GTC Taipai 2025 — Day 0：Building Digital Twins for Physical AI With NVIDIA Omniverse](../6560b300f03c/)
- [【Omniverse Farm】Omniverse Farm學習筆記 — 00：OVX](../0fdb74ed85ce/)
- [【Omniverse Farm】學習筆記 — 01：Using Movie Capture to Render](../1745b2c60c07/)
- [【Omniverse】學習筆記 — 00：Kit\-based Development](../a54ae65f22bf/)



_[Post](https://medium.com/maochinn/%E5%B7%A5%E4%BD%9C-nvidia-gtc-taipai-2025-%E5%87%BA%E5%B7%AE%E7%B4%80%E9%8C%84-4733ef5de7c8){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
