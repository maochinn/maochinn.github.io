<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="【筆記】Camera" /><meta name="author" content="帽捲" /><meta property="og:locale" content="en" /><meta name="description" content="這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。" /><meta property="og:description" content="這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。" /><link rel="canonical" href="https://medium-to-jekyll-starter.zhgchg.li//posts/dee562610e71/" /><meta property="og:url" content="https://medium-to-jekyll-starter.zhgchg.li//posts/dee562610e71/" /><meta property="og:site_name" content="Medium To Jekyll Starter" /><meta property="og:image" content="https://medium-to-jekyll-starter.zhgchg.li//assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-03-05T18:32:33+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://medium-to-jekyll-starter.zhgchg.li//assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" /><meta property="twitter:title" content="【筆記】Camera" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@帽捲" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"帽捲"},"dateModified":"2021-03-16T13:21:41+08:00","datePublished":"2021-03-05T18:32:33+08:00","description":"這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。","headline":"【筆記】Camera","image":"https://medium-to-jekyll-starter.zhgchg.li//assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://medium-to-jekyll-starter.zhgchg.li//posts/dee562610e71/"},"url":"https://medium-to-jekyll-starter.zhgchg.li//posts/dee562610e71/"}</script><title>【筆記】Camera | Medium To Jekyll Starter</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Medium To Jekyll Starter"><meta name="application-name" content="Medium To Jekyll Starter"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">Medium To Jekyll Starter</a><p class="site-subtitle fst-italic mb-0">A text-focused Jekyll theme</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/github_username" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['example','domain.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>【筆記】Camera</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>【筆記】Camera</h1><p class="post-desc fw-light mb-4">這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1614940353" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 5, 2021 </time> </span> <span> Updated <time data-ts="1615872101" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Mar 16, 2021 </time> </span><div class="mt-3 mb-3"> <a href="/assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" class="popup img-link preview-img shimmer"><img src="/assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="4351 words" > <em>24 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">【筆記】Camera</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">【筆記】Camera</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><h3 id="筆記camera"><span class="me-2">【筆記】Camera</span><a href="#筆記camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>這邊紀錄一下Pytorch3D的camera，這篇會順便把一些camera的東西順便複習一下。</p><p>眾所周知，電腦圖學，尤其是OpenGL實作的相機都是使用pinhole camera(針孔相機)，這在很多書都查的到，但是第一個問題是為甚麼採用pinhole，或者說pinhole的優點、缺點是什麼？</p><p><a href="/assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*IHJWOtr4KXdlkbmp.png" alt="" loading="lazy"></a></p><p>這邊稍微推薦一下台大VFX跟清大的電腦視覺開放式課程，裡面都有講到。</p><p><a href="https://www.csie.ntu.edu.tw/~cyy/courses/vfx/20spring/news/" target="_blank"><strong>Digital Visual Effects</strong></a> <a href="https://www.csie.ntu.edu.tw/~cyy/courses/vfx/20spring/news/" target="_blank"><em>With the help of digital technology, visual effects have been widely used in film production lately. For example, up to…</em> www.csie.ntu.edu.tw</a></p><p><a href="https://ocw.nthu.edu.tw/ocw/index.php?page=course&amp;cid=125" target="_blank" class="img-link shimmer" ><img src="http://ocw.nthu.edu.tw:8888/ocw/upload/teacher/92/92.jpg" alt="" loading="lazy"></a></p><p>首先，為什麼現實中的相機不使用針孔成像而採用透鏡成像，因為針孔成像需要的曝光時間很久，所以我透過透鏡來聚焦光線進而縮短曝光時間，但是只要有聚焦，那就會產生depth of focus(DoF) (景深)，也就是在focal plane以外的地方都會變模糊，也就會產生近景、遠景變得模糊。反過來說，pinhole camera就不會有這種問題，他只會有曝光時間很久的問題，而這個問題在電腦中是不存在的，因為我們可以直接把物體上的顏色投影到pixel上，所以我們可以看到大部分的camera都是採用pinhole作為model。</p><p>值得一提的是，你可以發現OpenGL或是其他real-time的rendering都沒有景深就是這個原因，甚至是ray tracing的rendering也有可能沒有景深，所以為了模擬現實的camera照出來的效果，我們會特別修改model去產生景深甚至石直接在後處理的時候hack出景深。</p><p><a href="https://raytracing.github.io/" target="_blank"><strong>Ray Tracing in One Weekend Series</strong></a> <a href="https://raytracing.github.io/" target="_blank"><em>The Ray Tracing in One Weekend series of books are now available to the public for free online. They are now released…</em> raytracing.github.io</a></p><p>pinhole成像的圖片有個特點，就是perspective projection，這個在 <a href="https://medium.com/maochinn/%E9%9B%BB%E8%85%A6%E5%9C%96%E5%AD%B801-transformation-%E6%96%BD%E5%B7%A5%E4%B8%AD-ea46dedf01f9?source=---------10-----------------------" target="_blank">電腦圖學01-Transformation</a> 有講過，總之就是近大遠小的效果。</p><p>以OpenGL來舉例，我們可以複習一下</p><p><a href="/assets/dee562610e71/0*8Eqies-TqIZASvhg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*8Eqies-TqIZASvhg.png" alt="" loading="lazy"></a></p><p>簡單來說就是經過model-&gt;world-&gt;view-&gt;clip space-&gt;image space，但值得注意的是clip space-&gt;image space這段是OpenGL幫我們處理掉的，所以我們並不知道他做了甚麼，那這邊會產生一個疑問，為甚麼要多一個clip space，難道不能從view space-&gt;image space？</p><p>答案是肯定的，其實有另一種流派的perspective projection，甚至比GL的還常見，他的流程就是world-&gt;view-&gt;image，甚至是直接是world-&gt;image。</p><p>使用這種transformation一個有名的例子就是OpenCV</p><p><a href="/assets/dee562610e71/0*5M6AuZdYLOAX4H4W.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*5M6AuZdYLOAX4H4W.png" alt="" loading="lazy"></a></p><p>造成這種差異是因為在電腦視覺中，往往我們就只會有photo以及相機的參數，而在電腦圖學中我們有的是整個場景但沒有image，換句話說，computer graphics 處理的是3D-&gt;2D的過程，而電腦視覺處理的是2D-&gt;3D的過程，我們不需要做所謂的depth testing, alpha testing，所以也就不需要NDC或是其他space，只需要單純從world-&gt;image就行了。</p><p>接下來借用下面這篇來說明，詳細還請自己看原文</p><p><a href="https://amytabb.com/ts/2019_06_28/" target="_blank"><strong>Converting OpenCV cameras to OpenGL cameras.</strong></a> <a href="https://amytabb.com/ts/2019_06_28/" target="_blank"><em>Back to Tips and Tricks Table of Contents This post will cover the following scenario: you have the internal and…</em> amytabb.com</a></p><h3 id="opencv-cameracalibrated-cameras"><span class="me-2">OpenCV Camera(Calibrated Cameras)</span><a href="#opencv-cameracalibrated-cameras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/assets/dee562610e71/0*5Do78J-f-uzNMlbQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*5Do78J-f-uzNMlbQ.png" alt="" loading="lazy"></a></p><p>簡單來說，就是透過extrinsic matrix(外部矩陣) and intrinsic matrix(內部矩陣)來從world-&gt;view-&gt;image。</p><p><a href="/assets/dee562610e71/1*7-nq3AUpvuwoxZSEHfvlOg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*7-nq3AUpvuwoxZSEHfvlOg.png" alt="" loading="lazy"></a></p><h4 id="extrinsic-matrix"><span class="me-2">Extrinsic Matrix</span><a href="#extrinsic-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>簡單來說，就是view matrix，也就是說他也是由一個rotation + translation，只是通常GL是4*4，CV則是用3*4，也就是到view space的時候只剩3維向量。</p><h4 id="intrinsic-matrix"><span class="me-2">Intrinsic Matrix</span><a href="#intrinsic-matrix" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>他會直接從view-&gt;image，這也是一個3*3的矩陣，所以轉換完的是3維向量，但是最後一個向量是homogeneous，所以其實就是2維向量，也就是pixel的位置。intrinsic matrix裡面的參數就是focal length與principle point之類的東西，focal length(焦距)原則上就跟field of view(FoV)是等價的東西，principle point則是看你的焦點在image plane上的位置，這邊詳細就不多講了。</p><p><a href="/assets/dee562610e71/0*BzGgZ09fyzxoEeLu.jpg" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*BzGgZ09fyzxoEeLu.jpg" alt="" loading="lazy"></a></p><p>那為甚麼一個叫做內部一個叫做外部呢？因為像是focal length或是principle point都是相機本身內部的結構造成的，而相機的位置以及朝向則是相對來講是外部的參數，換句話說，為甚麼要切成這兩種矩陣，就是因為在大多數情況下這兩者是獨立控制的。</p><p>從這之後我會稱這種系統的camera為CV camera，這樣會比較統一。</p><h3 id="opengl-camera"><span class="me-2">OpenGL Camera</span><a href="#opengl-camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>接下來我們來複習一下GL的camera，並且觀察一下它與CV的不同。</p><p><a href="/assets/dee562610e71/0*axI33BTfEpM-kzF0.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*axI33BTfEpM-kzF0.png" alt="" loading="lazy"></a></p><p>整個流程就是model-&gt;world-&gt;camera(view) -&gt;NDC(clip) -&gt;image。</p><p><a href="/assets/dee562610e71/1*bS5eecf9hwl1KP931ONhAQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*bS5eecf9hwl1KP931ONhAQ.png" alt="" loading="lazy"></a></p><p>這邊詳細一樣就不贅述，但是這邊可以注意到它並不是直接從view-&gt;NDC，而是用了兩個矩陣，這是因為我們在GL常用的perspective projection其實包含兩個部分。</p><p>也就是先把視錐轉成長方形，然後在從長方形轉到正方形(NDC)，而前者的轉換類似於內部矩陣，而後者其實就是平行投影矩陣。也就是說我們常常使用的gluPerspective之所以不會直接拿來解釋而是直接照填就是因為它還蠻複雜的。</p><p>那最後還有一個問題就是從NDC-&gt;image的這段到底是甚麼，因為通常在GL的pipeline他都會幫你處理好，但是現在我們必須要知道整個轉換的過程才有辦法比較。</p><p><a href="/assets/dee562610e71/1*C52r-CdvxJ1imv86AizJcQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*C52r-CdvxJ1imv86AizJcQ.png" alt="" loading="lazy"></a></p><p>這是其中一種做法，總之，我們會需要多一個矩陣將東西從NDC轉換到image。那我們可以稍微比較一下GL與CV的差異，簡單來說就是GL多轉換到了NDC，那為甚麼要這麼做就是因為在NDC我們可以很容易的做許多testing跟一些操作，還有限制值域到一個固定的範圍，而轉換到NDC的成本很低，只需要多幾個4*4矩陣運算。</p><p>概念上，我們可以把GL除了view matrix以外的東西全部乘起來當作是intrinsic matrix，那在最後，我會稍微解釋一下要怎麼用兩種表示方式但能做出一樣的轉換。</p><h3 id="blender-camera"><span class="me-2">Blender Camera</span><a href="#blender-camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>這邊算是補充一下，原則上blender camera的定義跟OpenGL一樣。</p><p><a href="/assets/dee562610e71/1*pf02noV322TdnNUtAUB0BA.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*pf02noV322TdnNUtAUB0BA.png" alt="" loading="lazy"></a></p><p>然後可以注意到Blender的世界坐標系是Z軸朝上。</p><p><a href="/assets/dee562610e71/1*7n4aaLfMyYWpvXKk4QnsRQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*7n4aaLfMyYWpvXKk4QnsRQ.png" alt="" loading="lazy"></a></p><h3 id="pytorch3d-camera"><span class="me-2">Pytorch3D Camera</span><a href="#pytorch3d-camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>這個是pytorch3D的camera定義，比較大的差別是+Z是相機的朝向，在GL是-Z為相機的朝向。然後在image space上則是類似於window space(視窗空間)，左上角為原點，這個跟GL和CV不同。</p><p><a href="/assets/dee562610e71/0*-iRa5RM30DuQLQ9s.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*-iRa5RM30DuQLQ9s.png" alt="" loading="lazy"></a></p><p>這邊有趣的地方就是Pytorch3D有兩種Camera。</p><blockquote><p>FoVPerspectiveCameras</p></blockquote><blockquote><p>PerspectiveCameras</p></blockquote><p>或者你可以在code裡面看到他們的舊稱</p><blockquote><p>OpenGLPerspectiveCameras</p></blockquote><blockquote><p>SfMPerspectiveCameras</p></blockquote><p>這樣就一目了然了，前者是類似於GL的camera定義方式，後者則是類似於CV的定義方式，這邊的sfm表示structure from motion，也就是從一連串照片想辦法計算出每張照片的參數，而這些參數通常就是內部、外部參數，所以可以進一步計算出類似於CV camera的內部矩陣、外部矩陣。</p><p>我們可以來看看他們的參數設定</p><p><a href="https://github.com/facebookresearch/pytorch3d/blob/340662e98e97c5e105cf6570765d7bae3e6228bf/pytorch3d/renderer/cameras.py#L79" target="_blank" class="img-link shimmer" ><img src="https://opengraph.githubassets.com/cd79cb3b16f6c64e9decdd248313d412ce8eaf31f2f6e7f5684a89576004a24c/facebookresearch/pytorch3d" alt="" loading="lazy"></a></p><h4 id="fovperspectivecameras"><span class="me-2">FoVPerspectiveCameras</span><a href="#fovperspectivecameras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊如果有學過GL應該就會知道參數要怎麼填，原則上就是要填view matrix跟perspective projection matrix。只是這邊可以注意到，K就是view-&gt;NDC的矩陣，所以如果我們能夠預先算出來的話就不用填znear, zfar, fov, aspective ratio，用GL的話說就是我們如果自己算好projection matrix我們就直接glLoadMatrix，不需要再用gluPerspective。</p><p><a href="/assets/dee562610e71/1*5ZWKbum7EEibCsGn9RjESA.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*5ZWKbum7EEibCsGn9RjESA.png" alt="" loading="lazy"></a></p><p>然後這邊的R跟T就是view matrix的rotation matrix以及translate matrix，這邊要提醒一下，view matrix是world-&gt;view，所以T並不是camera在world space的位置，但如果做個反矩陣就變成view-&gt;world，那此時的T就是camera在world space的位置。</p><p><a href="/assets/dee562610e71/1*aGg8ZhAfTaIxGp2lk1sYHA.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*aGg8ZhAfTaIxGp2lk1sYHA.png" alt="" loading="lazy"></a></p><p>最後我們可以來看一下K的計算方式，原則上對應的就是OpenGL perspective projection matrix的填法，只有兩點要注意，他這邊的fov是vertical fov，也就是垂直軸的視角，不要填成水平軸的視角除非你是正方形，那就沒差；第二個是在[3] [2]的位置的值是+1，而GL是-1，這是因為GL是-Z朝前，Pytorch3D則是+Z朝前。</p><h4 id="perspectivecameras"><span class="me-2">PerspectiveCameras</span><a href="#perspectivecameras" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊在外部參數的部分跟FoVPerspectiveCameras是一樣的，就是R跟T。</p><p><a href="/assets/dee562610e71/1*qjtQRI6VvkKyWzNOqXaTKg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*qjtQRI6VvkKyWzNOqXaTKg.png" alt="" loading="lazy"></a></p><p>那唯一的不同就是K矩陣的定義，這邊要注意的是，一般來說CV camera的K是view-&gt;image，但是在pytorch3D我們仍然需要做一些testing，所以這邊的K是view-&gt;image。</p><p><a href="/assets/dee562610e71/1*R3dFw87kkAtYxbkk0-UDfg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*R3dFw87kkAtYxbkk0-UDfg.png" alt="" loading="lazy"></a></p><p>這邊的focal length有兩種單位，一個正常的單位，另一個是以pixel為單位，舉例來說，我們可以設定focal length為1mm或是1px，如果是後者的話你必須在給你的image resolution。</p><p><a href="/assets/dee562610e71/1*Ru2VMGxyB68RM7aNp_eSjg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*Ru2VMGxyB68RM7aNp_eSjg.png" alt="" loading="lazy"></a></p><p>這邊我自己有個小疑問就是我們要怎麼將focal length轉換成fov，原則上單獨的focal length並不能直接轉換成fov，你必須要有sensor size，這跟image resolution並不一樣。</p><p><a href="/assets/dee562610e71/0*5rDgMypztItqAcnA" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*5rDgMypztItqAcnA" alt="" loading="lazy"></a></p><p>以真實的相機來說，我們的成像平面叫做sensor，然後這個sensor size是一個定值在一個相機裡面，所以我們可以透過調整focal length來調整視角，但是如果只有focal length並不能決定fov，你可以想像sensor size越大，這個fov就會越大。</p><p>而image resolution跟sensor size原則上是獨立的，假設我的sensor size是10*10mm，但我的resolution可以是800*800, 也可以是1920*1080，這只是差在我們怎麼去分割這10*10mm的sensor，所以可以算出來每個pixel的大小，例如前者每個piexl的大小是(10/800, 10/800)mm，後者則是(10/1920, 10/1080)，這可以發現，pixel不一定是正方的，有可能是長方的。</p><p>但是你可能會有疑問，為什麼CV camera的K裡面並沒有sensor size的參數，但是GL camera需要有完整的FOV資訊呢?我們可以先用簡單的case來說明。</p><p><a href="/assets/dee562610e71/0*QWZtUsqn43Hnqxmz" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*QWZtUsqn43Hnqxmz" alt="" loading="lazy"></a></p><p>假設我們的focal length為0.5mm，我們原點就在圖片中心，所以priciple point為(0, 0)，那麼我們K就變成一個單純的scaling matrix，所以如果我們的在view space有一個點(0, 1, 1)那就會投影到(0, 0.5)的sensor上，(0, -0.5) -&gt;(0, -0.25)，(圖上為了示意所以負的在上面，正的在下面)，但是我們的sensor size是0.6mm，所以超出的部分就直接丟掉。</p><p>所以我們不需要在K裡面有sensor size的資訊，因為我們會直接丟掉。但在GL的projection我們要把sensor size(例如上面的+0.3~-0.3)塞到NDC( +1~-1)，所以一定要有sensor size的資訊，所以才需要FOV。那我們可以在看一個比較實際的例子。</p><p><a href="/assets/dee562610e71/0*4sBcaeGz39vef-ns" class="popup img-link shimmer"><img src="/assets/dee562610e71/0*4sBcaeGz39vef-ns" alt="" loading="lazy"></a></p><p>假設我們的image的resolution是1920*1920px，那我們的focal length則是100px，那我們也可以將點對應到image plane上，所以只要我們確保focal length跟sensor size的單位是一致的就好了。</p><p>所以回到PerspectiveCameras的K，我們可以給image size如果我們的focal length的單位是px，那他預設的sensor size是[ -1, +1]，principle point是(0, 0)，也就是從view space-&gt;NDC，所以如果我們有fov也可以透過上面例子的那種方式反推我們要的focal length，實際上要怎麼計算我們下一節再介紹。</p><p><a href="https://pytorch3d.org/docs/cameras" target="_blank" class="img-link shimmer" ><img src="https://pytorch3d.org/img/pytorch3dlogoicon.svg" alt="" loading="lazy"></a></p><h3 id="camera-blender-to-pytorch3d"><span class="me-2">Camera: Blender to Pytorch3D</span><a href="#camera-blender-to-pytorch3d" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>為了從Blender的camera輸出到Pytorch3D，我們要先來搞懂Pytorch3D的Camera要怎麼使用，甚麼?上面不是介紹過了?這邊還有很多坑. . .，如果想跳過直接到Blender to Pytorch3D可以直接滑到下面。</p><p><a href="https://colab.research.google.com/drive/1ZBlX1O5kTDyei-hJ9xuVKGel_mv8i8QA?usp=sharing" target="_blank" class="img-link shimmer" ><img src="https://colab.research.google.com/img/colab_favicon_256px.png" alt="" loading="lazy"></a></p><p>這邊可以自己跑跑看上面的script，原則上就是拿Pytorch3D的教學來改，接下來會接著說明。</p><h4 id="install-and-import-modules"><span class="me-2">Install and Import modules</span><a href="#install-and-import-modules" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊就是安裝Pytorch3D，就照官方的走就好了。</p><h4 id="load-a-mesh-and-texture-file"><span class="me-2">Load a mesh and texture file</span><a href="#load-a-mesh-and-texture-file" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊也是官方的牛牛，就照著走就好了。</p><h4 id="create-a-renderer"><span class="me-2">Create a renderer</span><a href="#create-a-renderer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>這邊要建renderer，我們可以用LookAt的工具來產生rotation matrix跟translation matrix。這邊要注意Pytorch3D是採用row-major，可以在文件看到。</p><p><a href="/assets/dee562610e71/1*Cf9p0DbK792RwAx-K2kQXg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*Cf9p0DbK792RwAx-K2kQXg.png" alt="" loading="lazy"></a></p><p><a href="https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/row-major-vs-column-major-vector" target="_blank"><strong>Geometry (Row Major vs Column Major Vector)</strong></a> <a href="https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/row-major-vs-column-major-vector" target="_blank"><em>Earlier in this lesson, we have explained that vectors (or points) can be written down as [1x3] matrices (one row…</em> www.scratchapixel.com</a></p><p><a href="/assets/dee562610e71/1*Wk68NtmIJ8-nQryb_22uog.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*Wk68NtmIJ8-nQryb_22uog.png" alt="" loading="lazy"></a></p><p>這邊可以看到，這邊的[R|T]是(world to)view matrix，所以這邊的T的值並不是camera position in world space，而是world position in view space。以上圖微例，我們讓相機朝向(0, 0, 0)，而相機與其的距離為2.7，所以我們的camera實際上的位置是再(0, 0, -2.7)，如果要驗證的話可以拿[R|T]去做inverse，因為R是identity，所以我們直接把T加個負號就是inverse，也就是(0, 0, -2.7)，這就是camera position in world space。</p><p><a href="/assets/dee562610e71/1*9j7_mYSO0FauoEMKgUtMEQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*9j7_mYSO0FauoEMKgUtMEQ.png" alt="" loading="lazy"></a></p><p>我們透過RT就可以把fov camera建出來，這邊可以注意到projection matrix的1.7321其實就是focal length，順帶一提這邊預設的fov是60度。</p><p><a href="/assets/dee562610e71/1*0gcbAvsdzy2j9bCUmlERDw.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*0gcbAvsdzy2j9bCUmlERDw.png" alt="" loading="lazy"></a></p><p>sfm也可以用類似的方式建出來，這邊預設的focal length是1。</p><p><a href="/assets/dee562610e71/1*K6pycHMOQyU6q82U0Sk9vw.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*K6pycHMOQyU6q82U0Sk9vw.png" alt="" loading="lazy"></a></p><p><a href="/assets/dee562610e71/1*zqUF_z-eDhRFutVrOiSuPg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*zqUF_z-eDhRFutVrOiSuPg.png" alt="FoV(left), SfM(right)" loading="lazy"></a></p><p>FoV(left), SfM(right)</p><p>這邊可以發現fov camera的視角比較小，我們可以看到sfm camera的視角比較大，我們可以直接看兩者的projection matrix就可以發現前者的focal length比後者大。</p><h4 id="rendering-by-fov-camera-or-sfm-camera"><span class="me-2">Rendering by FoV Camera or SfM Camera</span><a href="#rendering-by-fov-camera-or-sfm-camera" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>那如果我們想要用兩種camera能夠render出相同的圖片，那我們就要改一下fov或是focal length。</p><p><a href="/assets/dee562610e71/1*--KwFE-0Tq4DahpiBN5XVQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*--KwFE-0Tq4DahpiBN5XVQ.png" alt="" loading="lazy"></a></p><p>這邊其實可以依據前面所講的可以把fov轉成focal length。</p><p><a href="/assets/dee562610e71/1*eciG6g4ueYajAZBKND8bAg.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*eciG6g4ueYajAZBKND8bAg.png" alt="" loading="lazy"></a></p><p>或是用更暴力的方法，直接拿fov的projection matrix丟進去，可是這邊就有一個疑問，他們的矩陣除了focal length一樣，其他都不一樣，這樣OK嗎?</p><h4 id="projection-and-unprojection"><span class="me-2">Projection and Unprojection</span><a href="#projection-and-unprojection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Pytorch3D提供了projection跟unprojection，前者表示從world space to NDC space，後者則是相反。首先我們可以先看fov camera</p><p><a href="/assets/dee562610e71/1*PyBN0V6ISlm7NvTPqxPXbA.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*PyBN0V6ISlm7NvTPqxPXbA.png" alt="" loading="lazy"></a></p><p>可以看到如果我們在NDC(0, 0, 0)做unproject到world space就是相機的位置world(0, 0, -2.7)，NDC(0, 0, 1)也就是在相機前面1單位(in NDC)，我們可以得到world(0, 0, -1.7)，其實這邊就有點奇怪了，照理來講通常NDC的一單位會大於world space的一單位，但是我們看下去. . .</p><p>如果我們從world(0, 0, 100) transform 到NDC(0, 0, 1)，就大約是在NDC的最遠了(z far)，然後world(0, 0, 0) -&gt;NDC(0, 0, 0.636)，world(0, 0, -1.7) -&gt;NDC(0, 0, 0)，這邊可以發現，距離相機越遠，在NDC裡的值越大，這非常合理，然後為甚麼world(0, 0, -1.7)會直接轉換到NDC的原點呢?因為我們znear=1，所以NDC的原點就是camera位置在往前1單位，所以world(0, 0, -2.7) -&gt;NDC(0, 0, -0.432)，拿相機原點去做反而會變成負值。</p><p>這邊另一個有趣的點是，NDC(0, 0, 1) -&gt;world(0, 0, -1.7)，但是world(0, 0, -1.7) -&gt;NDC(0, 0, 0)，這就表示這兩者的轉換並不是inverse的關係。這個我們可以在另一段code看到一些蛛絲馬跡。</p><p><a href="/assets/dee562610e71/1*2RFIevxsDW0SNjYvSf4zZw.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*2RFIevxsDW0SNjYvSf4zZw.png" alt="" loading="lazy"></a></p><p>這是MeshRasterizer的transform這邊很明顯的可以看到他直接把算出來的z值拿view space的z值蓋掉。</p><p>那我們在來看看相同的sfm camera。</p><p><a href="/assets/dee562610e71/1*MZ6ZoswO2Z5hytK3mUTbfQ.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*MZ6ZoswO2Z5hytK3mUTbfQ.png" alt="" loading="lazy"></a></p><p>這邊sfm並沒有znear，但是在NDC-&gt;world上跟fov camera相同，但是在world-&gt;NDC就不一樣了，可以看到距離越遠值反而越小，意思就是越遠的東西深度值越小，這非常反直覺，但是我們知道後來的z值會被直接蓋掉，所以其實這邊的z值並不是很重要。</p><p>所以這邊也回答了前面的問題，為什麼可以直接拿fov camera的K給sfm camera，因為他們差異的地方只會造成z值的變化，但因為之後都會被直接蓋掉，所以沒差(乾，害我想很久)。</p><h4 id="blender-to-pytorch3d"><span class="me-2">Blender to Pytorch3D</span><a href="#blender-to-pytorch3d" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>終於可以回到正題，先直接看看腳本吧。</p><p><a href="/assets/dee562610e71/1*k0XkJoj3tO9j_0Ad7yhqgw.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*k0XkJoj3tO9j_0Ad7yhqgw.png" alt="" loading="lazy"></a></p><p>首先，因為Pytorch3D的camera的參數就是R, T, K，所以這邊就要想辦法輸出內部、外部參數。</p><p>首先是外部參數，其實我們就是要算出pytorch3D world space to pytorch3D camera(view) space的矩陣就行，詳細之後會解釋，但這邊其實我是計算OpenGL world space to Pytorch3D camera space，因為GL world跟Pytorch3D world都是y朝上，所以我就直接用了，只是camera跟mesh都有轉到一樣的坐標系其實都可以。</p><p>那內部參數就是記得要拿vertical fov，因為pytorch的fov是vertical fov。</p><p><a href="/assets/dee562610e71/1*WIQnxfhdFJULVw-QNIRiqw.png" class="popup img-link shimmer"><img src="/assets/dee562610e71/1*WIQnxfhdFJULVw-QNIRiqw.png" alt="" loading="lazy"></a></p><p>這邊要記得sensor_fit要調成vertical他才會給你正確的vertical fov，然後我們可以計算出需要的focal length，然後把需要的矩陣都填出來，然後輸出就行了。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/maochinn/">Maochinn</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/computer-graphics/" class="post-tag no-text-decoration" >computer-graphics</a> <a href="/tags/pytorch3d/" class="post-tag no-text-decoration" >pytorch3d</a> <a href="/tags/blender/" class="post-tag no-text-decoration" >blender</a> <a href="/tags/opengl/" class="post-tag no-text-decoration" >opengl</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E3%80%90%E7%AD%86%E8%A8%98%E3%80%91Camera%20-%20Medium%20To%20Jekyll%20Starter&url=https%3A%2F%2Fmedium-to-jekyll-starter.zhgchg.li%2F%2Fposts%2Fdee562610e71%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E3%80%90%E7%AD%86%E8%A8%98%E3%80%91Camera%20-%20Medium%20To%20Jekyll%20Starter&u=https%3A%2F%2Fmedium-to-jekyll-starter.zhgchg.li%2F%2Fposts%2Fdee562610e71%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fmedium-to-jekyll-starter.zhgchg.li%2F%2Fposts%2Fdee562610e71%2F&text=%E3%80%90%E7%AD%86%E8%A8%98%E3%80%91Camera%20-%20Medium%20To%20Jekyll%20Starter" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/83f3d470b656/">【自動化】學習筆記 — 01：硬體</a><li class="text-truncate lh-lg"> <a href="/posts/de28dc8582ec/">【自動化】學習筆記 — 00</a><li class="text-truncate lh-lg"> <a href="/posts/6560b300f03c/">【工作】NVIDIA GTC Taipai 2025 — Day 0：Building Digital Twins for Physical AI With NVIDIA Omniverse</a><li class="text-truncate lh-lg"> <a href="/posts/230d146467b3/">【工作】NVIDIA GTC Taipai 2025 — Day 1：Manufacturing sessions</a><li class="text-truncate lh-lg"> <a href="/posts/616f6e63db42/">【工作】NVIDIA GTC Taipai 2025 — Day 2：Physical Al and Robotics Sessions</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/computer-graphics/">computer-graphics</a> <a class="post-tag btn btn-outline-primary" href="/tags/omniverse/">omniverse</a> <a class="post-tag btn btn-outline-primary" href="/tags/digital-art/">digital-art</a> <a class="post-tag btn btn-outline-primary" href="/tags/nvidia/">nvidia</a> <a class="post-tag btn btn-outline-primary" href="/tags/blender/">blender</a> <a class="post-tag btn btn-outline-primary" href="/tags/krenz/">krenz</a> <a class="post-tag btn btn-outline-primary" href="/tags/business-trip/">business-trip</a> <a class="post-tag btn btn-outline-primary" href="/tags/siggraph/">siggraph</a> <a class="post-tag btn btn-outline-primary" href="/tags/%E9%9B%BB%E8%85%A6%E5%9C%96%E5%AD%B8/">電腦圖學</a> <a class="post-tag btn btn-outline-primary" href="/tags/cosmos/">cosmos</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/93752bbd3f7/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1617092734" data-df="ll" > Mar 30, 2021 </time><h4 class="pt-0 my-2">【筆記】Frame Buffer Object</h4><div class="text-muted"><p>這邊是遇坑紀錄</p></div></div></a></article><article class="col"> <a href="/posts/4dae8b8db6ad/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1612184217" data-df="ll" > Feb 1, 2021 </time><h4 class="pt-0 my-2">【Pytorch3D】介紹</h4><div class="text-muted"><p>Pytorch3D是用來幫助做一些3D資料的pytorch，簡單來說就是讓我們可以對3D資料做一些常見的operation。那為甚麼FAIR(Facebook AI…</p></div></div></a></article><article class="col"> <a href="/posts/5a9ddb95cdc7/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1610702662" data-df="ll" > Jan 15, 2021 </time><h4 class="pt-0 my-2">【AR】OpenGL + OpenCV</h4><div class="text-muted"><p>//搬運中...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/c6779c24822b/" class="btn btn-outline-primary" aria-label="Older" ><p>【Paper】Motion Path Editing</p></a> <a href="/posts/93752bbd3f7/" class="btn btn-outline-primary" aria-label="Newer" ><p>【筆記】Frame Buffer Object</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://twitter.com/username">your_full_name</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.3.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.<br/> Automatically sync posts from Medium with <a href="https://zhgchg.li/posts/en-medium-to-jekyll/" target="_blank">ZhgChg.Li</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/computer-graphics/">computer-graphics</a> <a class="post-tag btn btn-outline-primary" href="/tags/omniverse/">omniverse</a> <a class="post-tag btn btn-outline-primary" href="/tags/digital-art/">digital-art</a> <a class="post-tag btn btn-outline-primary" href="/tags/nvidia/">nvidia</a> <a class="post-tag btn btn-outline-primary" href="/tags/blender/">blender</a> <a class="post-tag btn btn-outline-primary" href="/tags/krenz/">krenz</a> <a class="post-tag btn btn-outline-primary" href="/tags/business-trip/">business-trip</a> <a class="post-tag btn btn-outline-primary" href="/tags/siggraph/">siggraph</a> <a class="post-tag btn btn-outline-primary" href="/tags/%E9%9B%BB%E8%85%A6%E5%9C%96%E5%AD%B8/">電腦圖學</a> <a class="post-tag btn btn-outline-primary" href="/tags/cosmos/">cosmos</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
